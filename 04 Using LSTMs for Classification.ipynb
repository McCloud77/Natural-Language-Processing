{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LSTMs to Classify the 20 Newsgroups Data Set\n",
    "The 20 Newsgroups data set is a well known classification problem. The goal is to classify which newsgroup a particular post came from.  The 20 possible groups are:\n",
    "\n",
    "`comp.graphics\n",
    "comp.os.ms-windows.misc\n",
    "comp.sys.ibm.pc.hardware\n",
    "comp.sys.mac.hardware\n",
    "comp.windows.x\trec.autos\n",
    "rec.motorcycles\n",
    "rec.sport.baseball\n",
    "rec.sport.hockey\t\n",
    "sci.crypt\n",
    "sci.electronics\n",
    "sci.med\n",
    "sci.space\n",
    "misc.forsale\t\n",
    "talk.politics.misc\n",
    "talk.politics.guns\n",
    "talk.politics.mideast\t\n",
    "talk.religion.misc\n",
    "alt.atheism\n",
    "soc.religion.christian`\n",
    "\n",
    "As you can see, some pairs of groups may be quite similar while others are very different.\n",
    "The full dataset is given as a designated training set of size 11314 and test set of size 7532. The 20 categories are represented in roughly equal proportions, so the baseline accuracy is around 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps include loading in the 20 newsgroups data (or parts of it), loading in the GloVe data, building the word embedding matrix, and building the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import keras\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the newsgroups and store names in variable `categories`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "categories = list(newsgroups_train.target_names)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a random subset of newsgroups for further analysis and load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['talk.politics.guns', 'comp.sys.ibm.pc.hardware', 'talk.politics.misc', 'alt.atheism']\n"
     ]
    }
   ],
   "source": [
    "def select_number_of_categories(n, cats):\n",
    "    assert n > 0 and n <= 20, 'Incorrect number'\n",
    "    idx = np.random.choice(len(cats), int(n), replace=False)\n",
    "    selected_cats = [cats[i] for i in idx]\n",
    "    return selected_cats\n",
    "\n",
    "subcats = select_number_of_categories(4, categories)\n",
    "print(subcats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download categories from newsgroups data\n",
    "def load_categories(cats):\n",
    "    \"\"\"\n",
    "    The keyword 'remove' is used to strip metadata that may\n",
    "    lead to overfitting.\n",
    "    \"\"\"\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                          remove=('headers', 'footers', 'quotes'),\n",
    "                                          categories=cats)\n",
    "    newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                         remove=('headers', 'footers', 'quotes'),\n",
    "                                         categories=cats)\n",
    "    return newsgroups_train, newsgroups_test\n",
    "\n",
    "newsgroups_train, newsgroups_test = load_categories(subcats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles in training: 2081\n",
      "Number of articles in test: 1385\n",
      "Example of an article:\n",
      "Ahhh, remember the days of Yesterday?  When we were only \n",
      "\tgoing to pay $17 / month?\n",
      "\n",
      "\tWhen only 1.2% of the population would pay extra taxes?\n",
      "\n",
      "\tRemember when a few of us predicted that it wasn't true?  :)\n",
      "\tRemember the Inaugural?   Dancing and Singing!  Liberation\n",
      "\tat last!  \n",
      "\n",
      "\tWell, figure *this* out:\n",
      "\n",
      "\t5% VAT, estimated to raise $60-100 Billion per year ( on CNN )\n",
      "\tWork it out, chum...\n",
      "\n",
      "\t     $60,000,000,000  /  125,000,000 taxpayers = $480 / year\n",
      "\n",
      "        But, you exclaim, \" I'll get FREE HEALTH CARE! \"\n",
      "\tBut, I exclaim, \" No, you won't! \"\n",
      "\n",
      "\tThis is only for that poor 37 million who have none.  Not for\n",
      "\tYOU, chum. :)  That comes LATER.\n",
      "\n",
      "\tAdd in the estimates of the energy tax costs - $300-500 / year\n",
      "\n",
      "\tPlus, all that extra \"corporate and rich\" taxes that will \n",
      "\ttrickle down, and what do you have?\n",
      "\n",
      "\t$1,000 / year, just like I said two months ago.\n",
      "\n",
      "\tAnd, the best part?   You don't GET ANYTHING for it.\n",
      "\n",
      "\tDeficit is STILL projected to rise at same rate it's  been\n",
      "\trising at, by CLINTON'S OWN ESTIMATES.  And this assumes that\n",
      "\this plan WILL WORK!\n",
      "\n",
      "\tI mean, come on, it doesn't take a ROCKET SCIENTIST to see\n",
      "\tthat in another 2 or 3 years, we're GETTING ANOTHER WHOPPING\n",
      "\tTAX INCREASE, because the deficit will STILL be GROWING \n",
      "\tFASTER THAN the ECONOMY.\n",
      "\n",
      "\tAll Clinton is doing, is moving us to a HIGHER diving board.\n",
      "\n",
      "        Face it.  Clinton is Bush X 2.  In four more years, our\n",
      "\tcountry will be completely bankrupt, and your children's\n",
      "\tfuture, so oft mentioned by Pal Bill, will be gone.\n",
      "\n",
      "\tAnd those of you still deluding yourselves will be faced\n",
      "\twith the guilt.\n",
      "\n",
      "\tWell, <glancing at watch>, gotta go.  I want to be out of\n",
      "\there by noon.  Got an appointment at the lake.  No tax\n",
      "\tthere, yet.\n"
     ]
    }
   ],
   "source": [
    "print('Number of articles in training:', len(newsgroups_train.data))\n",
    "print('Number of articles in test:', len(newsgroups_test.data))\n",
    "print('Example of an article:') \n",
    "print(newsgroups_train.data[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 4 categories: 0.7898916967509025\n"
     ]
    }
   ],
   "source": [
    "def tfidf(train, test):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    vectors = vectorizer.fit_transform(train.data)\n",
    "    vectors_test = vectorizer.transform(test.data)\n",
    "\n",
    "    clf = MultinomialNB(alpha=.01)\n",
    "    clf.fit(vectors, train.target)\n",
    "\n",
    "    pred = clf.predict(vectors_test)\n",
    "    #metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
    "    acc = metrics.accuracy_score(test.target, pred)\n",
    "    return acc\n",
    "\n",
    "acc = tfidf(newsgroups_train, newsgroups_test)\n",
    "print('Accuracy with', len(subcats), 'categories:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results of Naive Bayes classifier with a simple RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum document length: 10446\n"
     ]
    }
   ],
   "source": [
    "print('Maximum document length:', max([len(doc.split(\" \")) for doc in newsgroups_train.data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 50000\n",
    "seq_length = 30\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(newsgroups_train.data)\n",
    "sequences_test = tokenizer.texts_to_sequences(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27920 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = pad_sequences(sequences_train, maxlen=seq_length)\n",
    "x_test = pad_sequences(sequences_test, maxlen=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2515,    18,    15, ...,    18,   549,  1661],\n",
       "       [ 1016,   508,    18, ...,  5233,     5,  5234],\n",
       "       [    8,  7951,   327, ...,  5235,     7,  4750],\n",
       "       ...,\n",
       "       [   20,  2885,   315, ..., 27913, 13764,  1373],\n",
       "       [ 2537,     3,  2619, ...,  3659,    25,  2276],\n",
       "       [   43,     4, 13757, ...,   307,   706,   813]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(np.asarray(newsgroups_train.target))\n",
    "y_test = keras.utils.to_categorical(np.asarray(newsgroups_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Glove pre-trained word vectors can be downloaded from:\n",
    "\n",
    "http://nlp.stanford.edu/data/glove.6B.zip (822 MB!)\n",
    "\n",
    "We will use the file `glove.6B.200d.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove/glove.6B.200d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a word embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.3791e-01, -4.7601e-01, -5.6369e-02, -3.9082e-01, -1.7544e-01,\n",
       "       -6.2244e-01, -3.9816e-01,  2.9620e-01, -6.0647e-02, -6.7017e-02,\n",
       "        1.1466e-01, -3.3015e-01, -2.0318e-02,  6.0616e-01, -1.3920e-01,\n",
       "        1.3896e-01, -5.4781e-01,  3.0864e-01,  1.7354e-01,  3.9927e-01,\n",
       "        2.1137e-01,  1.3004e+00,  8.8030e-01,  2.3946e-01,  2.8838e-01,\n",
       "       -4.6336e-01,  2.5745e-01, -3.1755e-01, -3.2877e-01, -5.9534e-01,\n",
       "        2.3983e-01,  3.4159e-01,  1.2754e-01, -8.8208e-01,  1.4258e-01,\n",
       "       -1.8857e-01, -1.6961e-01,  2.7808e-01, -2.4600e-01,  1.9122e-01,\n",
       "        5.0244e-01,  5.3660e-01, -5.3568e-01,  2.4827e-01,  3.2561e-01,\n",
       "        6.7882e-01,  9.6401e-01, -2.8892e-01,  5.1206e-01,  5.8496e-01,\n",
       "       -3.1934e-02, -2.4849e-02,  8.8564e-02,  1.7360e-01,  5.4166e-01,\n",
       "       -8.6743e-02, -3.8412e-01,  1.3974e-01, -7.4122e-03,  9.2210e-01,\n",
       "       -2.5799e-01, -4.7018e-01, -5.5742e-01, -2.1213e-02, -7.1072e-01,\n",
       "        8.0995e-02, -4.7254e-01, -3.2925e-01,  6.8052e-01,  1.7242e-01,\n",
       "        8.7783e-02, -2.6560e-01, -6.0070e-01, -8.5217e-02, -3.6977e-02,\n",
       "       -3.6593e-01, -6.2576e-01, -3.4162e-01,  5.4672e-02, -1.1734e-01,\n",
       "        1.9686e-01,  8.3758e-02,  4.3157e-01, -8.2195e-01, -5.7756e-01,\n",
       "        6.7821e-02, -4.9520e-01,  1.4769e-01,  3.2863e-01, -1.0649e+00,\n",
       "       -3.9756e-01, -3.4890e-01, -6.1548e-02,  7.5400e-01,  5.2457e-01,\n",
       "        1.3657e-01, -4.1904e-02, -4.4660e-01,  8.4754e-02,  3.7516e-01,\n",
       "       -6.2374e-02, -8.1762e-02, -4.1776e-01,  3.0157e-02, -7.7967e-01,\n",
       "        8.7627e-02,  9.0542e-02,  7.5266e-01,  2.9235e-02, -1.8324e-01,\n",
       "        5.5433e-01, -3.4632e-01, -7.8019e-02, -1.2078e-01, -6.8377e-01,\n",
       "        2.8826e-02, -4.1618e-01,  2.2341e-01, -8.0811e-01, -5.9707e-01,\n",
       "        4.6835e-01, -3.8246e-01, -2.3549e-01, -6.2565e-01,  6.1201e-01,\n",
       "        1.4221e-01,  2.4076e-02,  4.2106e-01,  5.1978e-01,  2.7811e-01,\n",
       "        1.6885e-01, -4.9293e-01,  3.1563e-01, -5.9663e-01,  1.5091e-01,\n",
       "       -6.3300e-01,  1.1324e-03, -6.2031e-02, -3.8694e-02, -2.6038e-01,\n",
       "        2.1907e-01,  2.3103e-01,  8.2427e-01,  1.4963e-01,  8.5767e-01,\n",
       "        1.5706e-01, -2.9116e-01, -4.2033e-01,  4.5080e-01,  3.9614e-01,\n",
       "       -2.0271e-01,  1.0702e+00, -4.1153e-01,  2.2282e-01,  1.3287e-01,\n",
       "        9.3896e-01,  1.6088e-01, -2.2247e-01, -1.1443e+00, -5.0556e-01,\n",
       "       -1.9619e-01,  3.4685e-01,  5.9883e-01,  2.7666e-02, -2.1223e-01,\n",
       "       -5.4970e-01, -1.6784e-01,  5.2375e-01, -9.8196e-02,  1.5559e-01,\n",
       "       -2.3997e-01, -1.1526e-01,  1.6577e-02,  6.1643e-02,  9.2234e-02,\n",
       "        2.5817e-02,  2.9163e-01, -5.0848e-01, -1.5164e-01,  1.8360e-01,\n",
       "        1.6434e+00, -3.1567e-01, -5.3165e-01, -4.4914e-01, -7.2425e-01,\n",
       "       -4.1122e-01,  1.8799e-01,  1.6130e-01,  7.4169e-01, -2.1597e-02,\n",
       "       -3.7775e-01, -5.5265e-01,  7.3535e-02,  8.5621e-02, -6.6452e-02,\n",
       "        5.4982e-02, -4.0218e-01,  4.3235e-01, -7.5727e-02,  7.0530e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_vec = embeddings_index['dog']\n",
    "dog_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create a matrix where the $i$th row gives the word embedding for the word represented by integer $i$.  \n",
    "Essentially, these will be the \"weights\" for the Embedding Layer.  \n",
    "Rather than learning the weights, we will use these ones and \"freeze\" the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 200))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (27921, 200)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of embedding matrix:', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Layer\n",
    "`keras.layers.recurrent.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)`\n",
    "\n",
    "- Similar in structure to the `SimpleRNN` layer\n",
    "- `units` defines the dimension of the recurrent state\n",
    "- `recurrent_...` refers the recurrent state aspects of the LSTM\n",
    "- `kernel_...` refers to the transformations done on the input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 200)           5584200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 30, 30)            27720     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 30)            7320      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30)                7320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 124       \n",
      "=================================================================\n",
      "Total params: 5,626,684\n",
      "Trainable params: 42,484\n",
      "Non-trainable params: 5,584,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_dimension = 200  # This is the dimension of the words we are using from GloVe\n",
    "dropout = 0.1\n",
    "rec_dropout = 0.1\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                    word_dimension,\n",
    "                    weights=[embedding_matrix],  # We set the weights to be the word vectors from GloVe\n",
    "                    input_length=seq_length,\n",
    "                    trainable=False))  # By setting trainable to False, we \"freeze\" the word embeddings.\n",
    "\n",
    "model.add(LSTM(seq_length, return_sequences=True, dropout=dropout, recurrent_dropout=rec_dropout))\n",
    "model.add(LSTM(seq_length, return_sequences=True, dropout=dropout, recurrent_dropout=rec_dropout))\n",
    "model.add(LSTM(seq_length, dropout=dropout, recurrent_dropout=rec_dropout))\n",
    "#model.add(Dense(20, activation='softmax'))\n",
    "model.add(Dense(len(subcats), activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(lr =0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2081 samples, validate on 1385 samples\n",
      "Epoch 1/20\n",
      "2081/2081 [==============================] - 14s 7ms/step - loss: 1.1803 - acc: 0.4277 - val_loss: 1.0691 - val_acc: 0.4462\n",
      "Epoch 2/20\n",
      "2081/2081 [==============================] - 8s 4ms/step - loss: 1.0071 - acc: 0.5310 - val_loss: 0.9862 - val_acc: 0.5025\n",
      "Epoch 3/20\n",
      "2081/2081 [==============================] - 9s 4ms/step - loss: 0.9503 - acc: 0.5608 - val_loss: 0.9633 - val_acc: 0.5531\n",
      "Epoch 4/20\n",
      "2081/2081 [==============================] - 9s 4ms/step - loss: 0.8921 - acc: 0.6103 - val_loss: 0.9064 - val_acc: 0.5964\n",
      "Epoch 5/20\n",
      "2081/2081 [==============================] - 8s 4ms/step - loss: 0.8337 - acc: 0.6434 - val_loss: 0.9015 - val_acc: 0.6029\n",
      "Epoch 6/20\n",
      "2081/2081 [==============================] - 8s 4ms/step - loss: 0.7867 - acc: 0.6646 - val_loss: 0.8982 - val_acc: 0.6245\n",
      "Epoch 7/20\n",
      "2081/2081 [==============================] - 8s 4ms/step - loss: 0.7165 - acc: 0.7011 - val_loss: 1.0756 - val_acc: 0.5798\n",
      "Epoch 8/20\n",
      "2081/2081 [==============================] - 8s 4ms/step - loss: 0.6851 - acc: 0.7054 - val_loss: 0.8861 - val_acc: 0.6289\n",
      "Epoch 9/20\n",
      "2081/2081 [==============================] - 8s 4ms/step - loss: 0.6558 - acc: 0.7371 - val_loss: 0.9780 - val_acc: 0.6079\n",
      "Epoch 10/20\n",
      "2081/2081 [==============================] - 9s 4ms/step - loss: 0.6246 - acc: 0.7463 - val_loss: 0.8556 - val_acc: 0.6433\n",
      "Epoch 11/20\n",
      "2081/2081 [==============================] - 9s 4ms/step - loss: 0.5844 - acc: 0.7660 - val_loss: 0.9027 - val_acc: 0.6361\n",
      "Epoch 12/20\n",
      "2081/2081 [==============================] - 9s 5ms/step - loss: 0.5519 - acc: 0.7751 - val_loss: 0.9045 - val_acc: 0.6563\n",
      "Epoch 13/20\n",
      "2081/2081 [==============================] - 8s 4ms/step - loss: 0.5244 - acc: 0.7943 - val_loss: 0.9044 - val_acc: 0.6505\n",
      "Epoch 14/20\n",
      "2081/2081 [==============================] - 8s 4ms/step - loss: 0.4882 - acc: 0.8087 - val_loss: 0.9541 - val_acc: 0.6404\n",
      "Epoch 15/20\n",
      "2081/2081 [==============================] - 8s 4ms/step - loss: 0.4871 - acc: 0.8087 - val_loss: 0.9179 - val_acc: 0.6527\n",
      "Epoch 16/20\n",
      "2081/2081 [==============================] - 8s 4ms/step - loss: 0.4456 - acc: 0.8260 - val_loss: 0.9366 - val_acc: 0.6549\n",
      "Epoch 17/20\n",
      "2081/2081 [==============================] - 8s 4ms/step - loss: 0.4363 - acc: 0.8366 - val_loss: 0.9799 - val_acc: 0.6412\n",
      "Epoch 18/20\n",
      "2081/2081 [==============================] - 8s 4ms/step - loss: 0.4019 - acc: 0.8481 - val_loss: 1.0060 - val_acc: 0.6592\n",
      "Epoch 19/20\n",
      "2081/2081 [==============================] - 7s 4ms/step - loss: 0.3863 - acc: 0.8530 - val_loss: 1.1073 - val_acc: 0.6440\n",
      "Epoch 20/20\n",
      "2081/2081 [==============================] - 8s 4ms/step - loss: 0.3471 - acc: 0.8683 - val_loss: 1.0463 - val_acc: 0.6693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b739739710>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=20, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
